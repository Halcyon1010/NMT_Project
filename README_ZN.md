# ä¸­è‹±æœºå™¨ç¿»è¯‘ (NMT) é¡¹ç›®

è¿™æ˜¯ä¸€ä¸ªåŸºäº PyTorch å®ç°çš„ **ä¸­æ–‡åˆ°è‹±æ–‡ (Zh-En)** ç¥ç»æœºå™¨ç¿»è¯‘é¡¹ç›®ã€‚é¡¹ç›®æ¶µç›–äº†RNN ã€Transformer ä»¥åŠé¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ (NLLB) çš„å®Œæ•´æµç¨‹ï¼Œæ—¨åœ¨å¯¹æ¯”ä¸åŒæ¶æ„åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

## ğŸ“‹ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®å®ç°äº†ä¸‰ç§ä¸»æµçš„ NMT è§£å†³æ–¹æ¡ˆï¼Œå¹¶æä¾›äº†ç»Ÿä¸€çš„æ•°æ®å¤„ç†ã€è®­ç»ƒã€æ¨ç†å’Œè¯„ä¼°æ¥å£ï¼š

1.  **RNN (Seq2Seq)**:
    * åŸºäº **GRU** çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚
    * å®ç°äº† **Luong Attention** æœºåˆ¶ (æ”¯æŒ `dot`, `general`, `concat` ä¸‰ç§å®ç°æ–¹å¼)ã€‚
    * æ”¯æŒ Teacher Forcing è®­ç»ƒç­–ç•¥ã€‚
2.  **Transformer**:
    * åŸºäº *Attention Is All You Need* çš„æ ‡å‡†æ¶æ„ã€‚
    * **æ¶ˆèå®éªŒæ”¯æŒ**ï¼šæ”¯æŒåˆ‡æ¢å½’ä¸€åŒ–å±‚ (`LayerNorm` vs `RMSNorm`) å’Œä½ç½®ç¼–ç  (`Sinusoidal` vs `Learnable`)ã€‚
3.  **NLLB (Fine-tuning)**:
    * åŸºäº Meta çš„ **No Language Left Behind (NLLB)** æ¨¡å‹ (`facebook/nllb-200-distilled-600M`) è¿›è¡Œå¾®è°ƒã€‚
    * ä½¿ç”¨ Hugging Face `transformers` åº“å®ç°ã€‚


## âš™ï¸ ç¯å¢ƒä¾èµ–
```bash
conda create -n NMT python=3.10.0
conda activate NMT

pip install -r requirements.txt
```
æ³¨æ„ï¼šé¦–æ¬¡è¿è¡Œä»£ç æ—¶ï¼Œè„šæœ¬ä¼šè‡ªåŠ¨ä¸‹è½½ NLTK çš„ punkt åˆ†è¯æ•°æ®åŒ…åˆ° `nltk_data/` ç›®å½•ã€‚


## ğŸ“Š æ•°æ®å‡†å¤‡

é¡¹ç›®ä¸»è¦æ”¯æŒ **JSONL æ ¼å¼** çš„æ•°æ®é›†ã€‚è¯·åœ¨ `data/` ç›®å½•ä¸‹å‡†å¤‡ä»¥ä¸‹æ–‡ä»¶ï¼š

- `train_100k.jsonl`ï¼ˆè®­ç»ƒé›†ï¼‰
- `valid.jsonl`ï¼ˆéªŒè¯é›†ï¼‰
- `test.jsonl`ï¼ˆæµ‹è¯•é›†ï¼‰

---

### æ•°æ®æ ¼å¼è¯´æ˜

æ¯ä¸€è¡ŒåŒ…å«ä¸€ä¸ª **JSON å¯¹è±¡**ï¼Œå¿…é¡»åŒ…å«ä»¥ä¸‹å­—æ®µï¼š

- `zh`ï¼šä¸­æ–‡æºå¥  
- `en`ï¼šè‹±æ–‡ç›®æ ‡å¥  

---

### æ•°æ®æ ¼å¼ç¤ºä¾‹

```json
{"zh": "æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†ã€‚", "en": "I love Natural Language Processing."}
{"zh": "æ·±åº¦å­¦ä¹ æ­£åœ¨æ”¹å˜ä¸–ç•Œã€‚", "en": "Deep learning is changing the world."}
```

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆQuick Startï¼‰

**æ³¨æ„ï¼š** ä»¥ä¸‹å‘½ä»¤ä¸­çš„è·¯å¾„ï¼ˆå¦‚ `--data_root`ï¼‰è¯·æ ¹æ®æ‚¨å®é™…çš„æ–‡ä»¶ä½ç½®è¿›è¡Œè°ƒæ•´ã€‚

---

### 1. è®­ç»ƒ RNN æ¨¡å‹ï¼ˆSeq2Seq + Attentionï¼‰

è®­ç»ƒä¸€ä¸ªåŸºäº **Luong Attentionï¼ˆGeneralï¼‰** çš„åŒå±‚ GRU æ¨¡å‹ï¼š

```bash
python train_RNN.py \
  --data_root ./data \
  --save_path ./results \
  --exp RNN_Luong_general \
  --hidden_size 512 \
  --n_layers 2 \
  --attn_method general \
  --batch_size 240 \
  --epochs 50 \
  --lr 8e-4
```

### 2. è®­ç»ƒ Transformer æ¨¡å‹

è®­ç»ƒä¸€ä¸ªæ ‡å‡† **Transformer** æ¨¡å‹ï¼ˆä½¿ç”¨ Learnable Positional Encoding å’Œ LayerNormï¼‰ï¼š

```bash
python train_transformer.py \
    --data_root ./data \
    --save_path ./results \
    --exp Transformer_base \
    --d_model 512 \
    --n_head 8 \
    --n_layers 6 \
    --ffn_dim 2048 \
    --norm_type layernorm \
    --pos_type learnable \
    --batch_size 120 \
    --epochs 50
```

### 3. å¾®è°ƒ NLLB æ¨¡å‹

ä½¿ç”¨ Hugging Face `transformers` å¾®è°ƒ `facebook/nllb-200-distilled-600M`

```bash
python Finetune_NLLB.py \
    --data_root ./data \
    --output_dir ./results/nllb_finetuned \
    --model_name_or_path facebook/nllb-200-distilled-600M \
    --train_file train_100k.jsonl \
    --valid_file valid.jsonl \
    --per_device_train_batch_size 32 \
    --gradient_accumulation_steps 4 \
    --num_train_epochs 3 \
    --learning_rate 1e-4
```
**æç¤º**ï¼šå¦‚æœæ˜¯æ˜¾å­˜è¾ƒå°ï¼Œè¯·å‡å° `per_device_train_batch_size` å¹¶å¢å¤§ `gradient_accumulation_steps`ã€‚

## ğŸ” æ¨ç†ä¸è¯„ä¼° (Inference & Evaluation)
### ç»Ÿä¸€è¯„ä¼°è„šæœ¬ (`inference.py`)

è¿™æ˜¯æ¨èçš„è¯„ä¼°æ–¹å¼ï¼Œå¯ä»¥åŒæ—¶åŠ è½½å¤šä¸ªæ¨¡å‹å¹¶åœ¨æµ‹è¯•é›†ä¸Šè®¡ç®— BLEU å’Œ BERTScoreã€‚
```bash
python inference.py \
    --data_root ./data \
    --test_file test.jsonl \
    --save_path ./inference_results \
    --models rnn transformer nllb \
    --rnn_ckpt ./results/RNN_Luong_general/best_model.pth \
    --trans_ckpt ./results/Transformer_base/best_model.pth \
    --nllb_path ./results/nllb_finetuned/final \
    --batch_size 100 \
    --beam_width 5 \
    --device cuda
```

### å•æ¨¡å‹æµ‹è¯•
å¦‚æœæ‚¨åªæƒ³æµ‹è¯•å•ä¸ªæ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨å¯¹åº”çš„æµ‹è¯•è„šæœ¬ï¼š

- **RNN:**
    ```bash
    python test_RNN.py \
    --resume ./results/RNN_Luong_general/best_model.pth \
    --decode beam \
    --beam_width 5
    ```

- **Transformer:**
    ```bash
    python test_transformer.py \
    --resume ./results/Transformer_base/best_model.pth \
    --decode beam \
    --beam_width 5
    ```

- **NLLB:**
    ```bash
    python test_NLLB.py \
    --model_path ./results/nllb_finetuned/final
    ```

## ğŸ›  æŠ€æœ¯ç»†èŠ‚

- **æ–‡æœ¬å¤„ç†ï¼š**
  - **ä¸­æ–‡ï¼š** ä½¿ç”¨ `jieba` è¿›è¡Œåˆ†è¯ã€‚
  - **è‹±æ–‡ï¼š** ä½¿ç”¨ `nltk.word_tokenize` è¿›è¡Œåˆ†è¯ã€‚
  - **è¯è¡¨ï¼š** è‡ªåŠ¨æ„å»ºå¹¶ä¿å­˜ä¸º `.pth` æ–‡ä»¶ï¼Œæ”¯æŒ `<pad>`ã€`<sos>`ã€`<eos>`ã€`<unk>` ç­‰ç‰¹æ®Š Tokenã€‚

- **è§£ç ç­–ç•¥ï¼š**
  - æ”¯æŒ **Greedy Search**ï¼ˆè´ªå©ªæœç´¢ï¼‰ã€‚
  - æ”¯æŒ **Beam Search**ï¼ˆæŸæœç´¢ï¼‰ï¼ŒåŒ…å«é•¿åº¦æƒ©ç½šï¼ˆLength Penaltyï¼‰æœºåˆ¶ï¼Œæ”¯æŒ Batch å¹¶è¡Œè§£ç ä»¥æé«˜æ¨ç†é€Ÿåº¦ã€‚

- **è¯„ä»·æŒ‡æ ‡ï¼š**
  - **BLEUï¼š** ä½¿ç”¨ `sacrebleu` åº“è®¡ç®—è¯­æ–™åº“çº§åˆ«çš„ BLEU åˆ†æ•°ã€‚
  - **BERTScoreï¼š** åˆ©ç”¨é¢„è®­ç»ƒ BERT æ¨¡å‹è®¡ç®—ç”Ÿæˆçš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚
  - **PPLï¼ˆPerplexityï¼‰ï¼š** ç”¨äºè¡¡é‡è¯­è¨€æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚


## ğŸ“‚ ç›®å½•ç»“æ„

```text
NMT_Project/
â”œâ”€â”€ data/                   # æ•°æ®é›†ç›®å½• (JSONL æ ¼å¼)
â”‚   â”œâ”€â”€ train_100k.jsonl
â”‚   â”œâ”€â”€ valid.jsonl
â”‚   â””â”€â”€ test.jsonl
â”œâ”€â”€ nltk_data/              # NLTK æ•°æ®ç¼“å­˜
â”œâ”€â”€ results/                # å®éªŒç»“æœä¸æ¨¡å‹æƒé‡ä¿å­˜ç›®å½•
â”œâ”€â”€ Finetune_NLLB.py        # NLLB å¾®è°ƒè„šæœ¬
â”œâ”€â”€ inference.py            # ç»Ÿä¸€æ¨ç†ä¸è¯„ä¼°è„šæœ¬
â”œâ”€â”€ nlp_dataset.py          # æ•°æ®é¢„å¤„ç†ä¸ Dataset å®šä¹‰
â”œâ”€â”€ RNN.py                  # RNN æ¨¡å‹æ¶æ„å®šä¹‰
â”œâ”€â”€ Transformer.py          # Transformer æ¨¡å‹æ¶æ„å®šä¹‰
â”œâ”€â”€ train_RNN.py            # RNN è®­ç»ƒè„šæœ¬
â”œâ”€â”€ train_transformer.py    # Transformer è®­ç»ƒè„šæœ¬
â”œâ”€â”€ test_RNN.py             # RNN æµ‹è¯•è„šæœ¬
â”œâ”€â”€ test_transformer.py     # Transformer æµ‹è¯•è„šæœ¬
â”œâ”€â”€ test_NLLB.py            # NLLB æµ‹è¯•è„šæœ¬
â””â”€â”€ utils.py                # å·¥å…·å‡½æ•° (Beam Search, Metrics, Logger)

